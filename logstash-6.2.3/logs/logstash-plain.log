[2018-04-24T10:30:32,778][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T10:30:32,839][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T10:30:33,008][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T10:30:33,476][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T10:30:33,895][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T10:30:36,355][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T10:30:36,608][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x313fda83 run>"}
[2018-04-24T10:30:36,700][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T10:31:00,975][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: org.jruby.exceptions.RaiseException: (LoadError) no such file to load -- D:/logstash-6.2.3/lib/sqlserverdriver/mssql-jdbc-6.2.2.jre8
[2018-04-24T10:32:42,520][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T10:32:42,534][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T10:32:42,689][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T10:32:43,131][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T10:32:43,444][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T10:32:45,771][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T10:32:46,059][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x4994659c run>"}
[2018-04-24T10:32:46,140][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T10:33:00,715][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: org.jruby.exceptions.RaiseException: (LoadError) no such file to load -- lib/sqlserverdriver/mssql-jdbc-6.2.2.jre8
[2018-04-24T10:42:59,571][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T10:42:59,585][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T10:42:59,748][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T10:43:00,188][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T10:43:00,560][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T10:43:02,800][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T10:43:03,209][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x622c026 run>"}
[2018-04-24T10:43:03,291][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T10:44:01,358][INFO ][logstash.inputs.jdbc     ] (0.045000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:44:01,821][INFO ][logstash.outputs.file    ] Opening file {:path=>"D:/vrv.txt"}
[2018-04-24T10:45:00,286][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:46:00,175][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:47:00,075][INFO ][logstash.inputs.jdbc     ] (0.004000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:48:00,271][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:49:00,168][INFO ][logstash.inputs.jdbc     ] (0.002000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:50:00,069][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:51:00,258][INFO ][logstash.inputs.jdbc     ] (0.002000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:52:00,162][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:53:00,054][INFO ][logstash.inputs.jdbc     ] (0.002000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:54:00,251][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:55:00,145][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:56:00,050][INFO ][logstash.inputs.jdbc     ] (0.004000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:57:00,241][INFO ][logstash.inputs.jdbc     ] (0.002000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:58:00,160][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:59:00,038][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T10:59:47,339][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T10:59:47,711][INFO ][logstash.outputs.file    ] Closing file D:/vrv.txt
[2018-04-24T10:59:47,781][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x622c026 run>"}
[2018-04-24T11:00:28,986][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:00:29,001][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:00:29,157][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:00:29,618][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:00:30,066][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:00:32,548][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:00:33,036][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x1a17e18b sleep>"}
[2018-04-24T11:00:33,112][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:01:01,270][INFO ][logstash.inputs.jdbc     ] (0.036000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T11:01:01,746][INFO ][logstash.outputs.file    ] Opening file {:path=>"D:/vrv.txt"}
[2018-04-24T11:02:00,268][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T11:03:00,149][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent
[2018-04-24T11:03:49,443][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:03:49,967][INFO ][logstash.outputs.file    ] Closing file D:/vrv.txt
[2018-04-24T11:03:50,035][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x1a17e18b run>"}
[2018-04-24T11:05:33,994][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:05:34,010][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:05:34,169][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:05:34,600][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:05:34,962][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:05:37,722][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_403519b0-ef36-4343-813b-d7526d99b9d3", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:05:37,871][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:05:38,321][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:05:38,329][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:05:38,477][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:05:38,565][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:05:38,568][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:05:38,581][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:05:38,598][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:05:38,637][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:05:39,087][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x7cf22b78 run>"}
[2018-04-24T11:05:39,168][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:06:01,284][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.last_modify_time >= :sql_last_start
[2018-04-24T11:06:01,314][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:07:00,329][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.last_modify_time >= :sql_last_start
[2018-04-24T11:07:00,332][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:08:00,222][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.last_modify_time >= :sql_last_start
[2018-04-24T11:08:00,224][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:09:00,114][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.last_modify_time >= :sql_last_start
[2018-04-24T11:09:00,116][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:09:59,472][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:10:00,875][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x7cf22b78 run>"}
[2018-04-24T11:10:27,525][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:10:27,540][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:10:27,708][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:10:28,165][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:10:28,516][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:10:31,465][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_b47d2eb2-7b54-4e46-b08e-18674dd053f0", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:10:31,643][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:10:31,969][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:10:31,976][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:10:32,124][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:10:32,169][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:10:32,172][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:10:32,184][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:10:32,201][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:10:32,235][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:10:32,658][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x3d7eefaa sleep>"}
[2018-04-24T11:10:32,755][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:11:01,122][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= :sql_last_start
[2018-04-24T11:11:01,148][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:12:00,155][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= :sql_last_start
[2018-04-24T11:12:00,157][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:12:16,511][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:12:17,312][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x3d7eefaa run>"}
[2018-04-24T11:12:50,944][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:12:50,959][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:12:51,127][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:12:51,555][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:12:51,929][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:12:54,517][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_4e6fff0f-e531-4a0e-8a0b-d8a6c40aa3a5", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:12:54,664][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:12:55,037][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:12:55,044][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:12:55,192][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:12:55,240][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:12:55,244][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:12:55,258][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:12:55,275][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:12:55,311][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:12:55,718][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x3de32658 run>"}
[2018-04-24T11:12:55,800][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:13:00,999][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “:”附近有语法错误。: SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= :sql_last_start
[2018-04-24T11:13:01,023][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â:âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T11:13:36,430][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:13:37,319][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x3de32658 run>"}
[2018-04-24T11:17:07,289][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:17:07,303][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:17:07,475][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:17:07,879][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:17:08,241][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:17:10,805][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_42c5c0b4-34f2-43fb-aedf-4bc359387109", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:17:10,964][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:17:11,345][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:17:11,354][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:17:11,510][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:17:11,558][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:17:11,562][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:17:11,575][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:17:11,592][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:17:11,629][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:17:12,029][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x11a99dac run>"}
[2018-04-24T11:17:12,094][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:18:01,188][INFO ][logstash.inputs.jdbc     ] (0.068000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '1970-01-01T08:00:00.000'
[2018-04-24T11:18:02,738][INFO ][logstash.outputs.file    ] Opening file {:path=>"D:/vrv.txt"}
[2018-04-24T11:19:00,324][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:18:00.904'
[2018-04-24T11:19:14,473][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:19:14,896][INFO ][logstash.outputs.file    ] Closing file D:/vrv.txt
[2018-04-24T11:19:15,723][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x11a99dac run>"}
[2018-04-24T11:22:06,672][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:22:06,688][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:22:06,867][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:22:07,293][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:22:07,619][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:22:10,369][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_148d99a2-4797-47ac-a896-e49bf91f7d2e", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:22:10,527][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:22:10,932][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:22:10,941][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:22:11,095][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:22:11,151][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:22:11,156][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:22:11,174][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:22:11,193][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:22:11,231][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:22:11,712][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x38ee1bf9 run>"}
[2018-04-24T11:22:11,794][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:23:01,165][INFO ][logstash.inputs.jdbc     ] (0.041000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:19:00.303'
[2018-04-24T11:23:46,784][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:23:47,294][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x38ee1bf9 run>"}
[2018-04-24T11:24:19,036][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:24:19,050][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:24:19,205][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:24:19,626][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:24:20,016][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:24:22,829][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"local", index=>"vrv", document_type=>"vrv", hosts=>[//localhost:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_4f257e38-0553-4182-9820-0cffa72ed214", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:24:22,985][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:24:23,380][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2018-04-24T11:24:23,388][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2018-04-24T11:24:23,539][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2018-04-24T11:24:23,582][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:24:23,586][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:24:23,599][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:24:23,615][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:24:23,659][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2018-04-24T11:24:24,084][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x4ddae2a1 sleep>"}
[2018-04-24T11:24:24,170][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:25:01,200][INFO ][logstash.inputs.jdbc     ] (0.040000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '1970-01-01T08:00:00.000'
[2018-04-24T11:25:03,753][INFO ][logstash.outputs.file    ] Opening file {:path=>"D:/vrv.txt"}
[2018-04-24T11:26:00,068][INFO ][logstash.inputs.jdbc     ] (0.003000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:25:00.941'
[2018-04-24T11:27:00,258][INFO ][logstash.inputs.jdbc     ] (0.004000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:26:00.053'
[2018-04-24T11:28:00,182][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:27:00.249'
[2018-04-24T11:29:00,081][INFO ][logstash.inputs.jdbc     ] (0.005000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:28:00.171'
[2018-04-24T11:29:12,615][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T11:29:13,912][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x4ddae2a1 run>"}
[2018-04-24T11:39:20,082][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:39:20,100][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:39:20,273][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:39:20,707][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:39:21,092][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:39:21,435][ERROR][logstash.agent           ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>"LogStash::ConfigurationError", :message=>"Expected one of #, input, filter, output at line 60, column 1 (byte 1423) after ", :backtrace=>["C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/compiler.rb:42:in `compile_imperative'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/compiler.rb:50:in `compile_graph'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/compiler.rb:12:in `block in compile_sources'", "org/jruby/RubyArray.java:2486:in `map'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/compiler.rb:11:in `compile_sources'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/pipeline.rb:51:in `initialize'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/pipeline.rb:169:in `initialize'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/pipeline_action/create.rb:40:in `execute'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:315:in `block in converge_state'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:141:in `with_pipelines'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:312:in `block in converge_state'", "org/jruby/RubyArray.java:1734:in `each'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:299:in `converge_state'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:166:in `block in converge_state_and_update'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:141:in `with_pipelines'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:164:in `converge_state_and_update'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/agent.rb:90:in `execute'", "C:/ELK/logstash-6.2.3/logstash-core/lib/logstash/runner.rb:348:in `block in execute'", "C:/ELK/logstash-6.2.3/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/task.rb:24:in `block in initialize'"]}
[2018-04-24T11:41:24,894][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:41:24,908][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:41:25,066][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:41:25,490][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:41:25,869][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:41:29,722][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"united-imaging", index=>"vrv", document_type=>"vrv", user=>"elastic", password=><password>, hosts=>[http://elastic.united-imaging.com:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_c25c08d2-7b96-4edd-badd-9ef5a7fb4462", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:41:29,800][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:41:30,227][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@elastic.united-imaging.com:9200/]}}
[2018-04-24T11:41:30,245][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://elastic:xxxxxx@elastic.united-imaging.com:9200/, :path=>"/"}
[2018-04-24T11:41:30,498][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://elastic:xxxxxx@elastic.united-imaging.com:9200/"}
[2018-04-24T11:41:30,549][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:41:30,552][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:41:30,566][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:41:30,592][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:41:30,637][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://elastic.united-imaging.com:9200"]}
[2018-04-24T11:41:31,107][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x57295a98 sleep>"}
[2018-04-24T11:41:31,196][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:42:00,718][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: org.jruby.exceptions.RaiseException: (LoadError) no such file to load -- D:/logstash-6.2.3/lib/sqlserverdriver/mssql-jdbc-6.2.2.jre8
[2018-04-24T11:42:53,631][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T11:42:53,644][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T11:42:53,818][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T11:42:54,308][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T11:42:54,652][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T11:42:58,300][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"united-imaging", index=>"vrv", document_type=>"vrv", user=>"elastic", password=><password>, hosts=>[http://elastic.united-imaging.com:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_dc670daf-d728-4664-8b96-82802e8bfbde", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T11:42:58,385][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T11:42:58,786][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@elastic.united-imaging.com:9200/]}}
[2018-04-24T11:42:58,805][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://elastic:xxxxxx@elastic.united-imaging.com:9200/, :path=>"/"}
[2018-04-24T11:42:59,031][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://elastic:xxxxxx@elastic.united-imaging.com:9200/"}
[2018-04-24T11:42:59,092][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T11:42:59,096][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T11:42:59,118][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T11:42:59,247][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T11:42:59,284][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://elastic.united-imaging.com:9200"]}
[2018-04-24T11:42:59,691][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0xcfebbc6 run>"}
[2018-04-24T11:42:59,855][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T11:44:01,262][INFO ][logstash.inputs.jdbc     ] (0.042000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '1970-01-01T08:00:00.000'
[2018-04-24T11:45:00,153][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:44:00.993'
[2018-04-24T11:46:00,344][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:45:00.136'
[2018-04-24T11:47:00,245][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:46:00.329'
[2018-04-24T11:48:00,141][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:47:00.231'
[2018-04-24T11:49:00,334][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:48:00.125'
[2018-04-24T11:50:00,234][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:49:00.321'
[2018-04-24T11:51:00,122][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:50:00.221'
[2018-04-24T11:52:00,331][INFO ][logstash.inputs.jdbc     ] (0.013000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:51:00.112'
[2018-04-24T11:53:00,209][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:52:00.316'
[2018-04-24T11:54:00,105][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:53:00.201'
[2018-04-24T11:55:00,304][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:54:00.096'
[2018-04-24T11:56:00,200][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:55:00.295'
[2018-04-24T11:57:00,134][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:56:00.192'
[2018-04-24T11:58:00,293][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:57:00.126'
[2018-04-24T11:59:00,245][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:58:00.284'
[2018-04-24T12:00:00,137][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T11:59:00.235'
[2018-04-24T12:01:00,035][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:00:00.128'
[2018-04-24T12:02:00,233][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:01:00.026'
[2018-04-24T12:03:00,130][INFO ][logstash.inputs.jdbc     ] (0.010000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:02:00.225'
[2018-04-24T12:04:00,326][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:03:00.119'
[2018-04-24T12:05:00,230][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:04:00.319'
[2018-04-24T12:06:00,121][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:05:00.221'
[2018-04-24T12:07:00,318][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:06:00.113'
[2018-04-24T12:08:00,217][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:07:00.311'
[2018-04-24T12:09:00,115][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:08:00.208'
[2018-04-24T12:10:00,316][INFO ][logstash.inputs.jdbc     ] (0.010000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:09:00.107'
[2018-04-24T12:11:00,203][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:10:00.304'
[2018-04-24T12:12:00,092][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:11:00.195'
[2018-04-24T12:13:00,285][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:12:00.083'
[2018-04-24T12:14:00,164][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:13:00.278'
[2018-04-24T12:15:00,058][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:14:00.156'
[2018-04-24T12:16:00,254][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:15:00.047'
[2018-04-24T12:17:00,143][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:16:00.247'
[2018-04-24T12:18:00,033][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:17:00.133'
[2018-04-24T12:19:00,242][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:18:00.024'
[2018-04-24T12:20:00,131][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:19:00.232'
[2018-04-24T12:21:00,314][INFO ][logstash.inputs.jdbc     ] (0.007000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:20:00.122'
[2018-04-24T12:22:00,195][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:21:00.306'
[2018-04-24T12:23:00,080][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:22:00.187'
[2018-04-24T12:24:00,279][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:23:00.073'
[2018-04-24T12:25:00,169][INFO ][logstash.inputs.jdbc     ] (0.008000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:24:00.271'
[2018-04-24T12:26:00,061][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:25:00.160'
[2018-04-24T12:27:00,249][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:26:00.053'
[2018-04-24T12:28:00,132][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:27:00.242'
[2018-04-24T12:29:00,322][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:28:00.125'
[2018-04-24T12:30:00,203][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:29:00.314'
[2018-04-24T12:31:00,093][INFO ][logstash.inputs.jdbc     ] (0.006000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:30:00.195'
[2018-04-24T12:32:00,279][INFO ][logstash.inputs.jdbc     ] (0.009000s) SELECT TOP (1000) * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:31:00.083'
[2018-04-24T12:32:49,715][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T12:32:51,055][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0xcfebbc6 run>"}
[2018-04-24T12:33:38,529][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T12:33:38,542][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T12:33:38,701][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T12:33:39,114][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T12:33:39,478][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T12:33:41,896][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"united-imaging", index=>"vrv", document_type=>"vrv", user=>"elastic", password=><password>, hosts=>[http://elastic.united-imaging.com:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_3d34ed26-0d63-47ff-bd9a-9785e78c23c1", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T12:33:41,962][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T12:33:42,338][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@elastic.united-imaging.com:9200/]}}
[2018-04-24T12:33:42,356][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://elastic:xxxxxx@elastic.united-imaging.com:9200/, :path=>"/"}
[2018-04-24T12:33:42,556][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://elastic:xxxxxx@elastic.united-imaging.com:9200/"}
[2018-04-24T12:33:42,617][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T12:33:42,620][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T12:33:42,634][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T12:33:42,650][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T12:33:42,688][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://elastic.united-imaging.com:9200"]}
[2018-04-24T12:33:43,074][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x29fbd9ce run>"}
[2018-04-24T12:33:43,141][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T12:34:01,153][ERROR][logstash.inputs.jdbc     ] Java::ComMicrosoftSqlserverJdbc::SQLServerException: “*”附近有语法错误。: SELECT TOP * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '1970-01-01T08:00:00.000'
[2018-04-24T12:34:01,181][WARN ][logstash.inputs.jdbc     ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::ComMicrosoftSqlserverJdbc::SQLServerException: â*âéè¿æè¯­æ³éè¯¯ã>}
[2018-04-24T12:34:49,296][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2018-04-24T12:34:49,716][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=>"main", :thread=>"#<Thread:0x29fbd9ce run>"}
[2018-04-24T12:35:29,268][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"C:/ELK/logstash-6.2.3/modules/fb_apache/configuration"}
[2018-04-24T12:35:29,282][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"C:/ELK/logstash-6.2.3/modules/netflow/configuration"}
[2018-04-24T12:35:29,445][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2018-04-24T12:35:29,852][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.2.3"}
[2018-04-24T12:35:30,192][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2018-04-24T12:35:32,810][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch id=>"united-imaging", index=>"vrv", document_type=>"vrv", user=>"elastic", password=><password>, hosts=>[http://elastic.united-imaging.com:9200], enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_1163a9fa-960e-4fe1-bb33-bb6a350061a8", enable_metric=>true, charset=>"UTF-8">, workers=>1, manage_template=>true, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2018-04-24T12:35:32,882][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2018-04-24T12:35:33,266][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@elastic.united-imaging.com:9200/]}}
[2018-04-24T12:35:33,284][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://elastic:xxxxxx@elastic.united-imaging.com:9200/, :path=>"/"}
[2018-04-24T12:35:33,486][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://elastic:xxxxxx@elastic.united-imaging.com:9200/"}
[2018-04-24T12:35:33,535][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2018-04-24T12:35:33,538][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2018-04-24T12:35:33,552][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2018-04-24T12:35:33,568][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2018-04-24T12:35:33,604][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://elastic.united-imaging.com:9200"]}
[2018-04-24T12:35:34,013][INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=>"main", :thread=>"#<Thread:0x6e31cbf run>"}
[2018-04-24T12:35:34,085][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
[2018-04-24T12:36:01,121][INFO ][logstash.inputs.jdbc     ] (0.039000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '1970-01-01T08:00:00.000'
[2018-04-24T12:37:00,324][INFO ][logstash.inputs.jdbc     ] (0.005000s) SELECT * FROM dbo.PMoveableDiskEvent where dbo.PMoveableDiskEvent.UpTime >= '2018-04-24T12:36:00.868'
